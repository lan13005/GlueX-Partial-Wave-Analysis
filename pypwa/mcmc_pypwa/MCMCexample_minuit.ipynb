{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EtaPi PWA\n",
    "based on demo_JPAC_fit.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "total=0\n",
    "n=9\n",
    "for k in range(1,n+1):\n",
    "    total+=math.comb(n,k)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPWA as pwa\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import pandas\n",
    "import numpy as npy\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import corner\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 26\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to stick to [epsilon][L][M]_[epsilon][L][M]_...\n",
    "outputpath = \"example\"\n",
    "try:\n",
    "    os.makedirs(outputpath)\n",
    "except OSError:\n",
    "    if os.path.isdir(outputpath):\n",
    "        print(f\"Directory {outputpath} already exists.\")\n",
    "    else:\n",
    "        print (f\"Creation of the directory {outputpath} failed.\")\n",
    "else:\n",
    "    print (f\"Successfully created the directory {outputpath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read processed data\n",
    "pre-processed by process_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_full = pwa.read(\"./zPhase1_data/phase1_data_data.csv\")\n",
    "# mc_full = pwa.read(\"./zPhase1_data/phase1_flat_data.csv\")\n",
    "\n",
    "# data_full_weight = pwa.read(\"./zPhase1_data/phase1_data_weights.csv\")\n",
    "# mc_full_weight = pwa.read(\"./zPhase1_data/phase1_flat_weights.csv\")\n",
    "\n",
    "baseFolder=\"/d/grid17/ln16/myDSelector/amptools/pypwa/mcmc_pypwa/zMalte_kmatrix_hel/\"\n",
    "data_full = pwa.read(baseFolder+\"malte_kmatrix_data.csv\")\n",
    "mc_full = pwa.read(baseFolder+\"flat_2018_8_data_000.csv\")\n",
    "\n",
    "data_full_weight = pwa.read(baseFolder+\"malte_kmatrix_weights.csv\")\n",
    "mc_full_weight = pwa.read(baseFolder+\"flat_2018_8_weights_000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weight=data_full_weight#[data_full[\"beamAngle\"]==0]\n",
    "data=data_full#[data_full[\"beamAngle\"]==0]\n",
    "\n",
    "mc_weight=mc_full_weight#[mc_full[\"beamAngle\"]==0]\n",
    "mc=mc_full#[mc_full[\"beamAngle\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data=pandas.DataFrame(mc)\n",
    "fig,ax=plt.subplots(1,1,figsize=(16,16))\n",
    "pd_data.hist(ax=ax,bins=100,weights=mc_weight[\"weightASBS\"])\n",
    "\n",
    "pd_data=pandas.DataFrame(data)\n",
    "fig,ax=plt.subplots(1,1,figsize=(16,16))\n",
    "pd_data.hist(ax=ax,bins=100,weights=data_weight[\"weightASBS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist2d(pd_data.mass,npy.cos(pd_data.theta),bins=(75,75),weights=data_weight[\"weightASBS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Waves for Fit (and input initial values of minuit and fitted parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit real and imaginary parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupWave(waveset):\n",
    "    Vs = {\"errordef\": 1}\n",
    "    initial = []\n",
    "    for waveConfig in waveset:\n",
    "        wave, anchor = waveConfig\n",
    "        for param_type in [\"r\", \"i\"]:\n",
    "            initial.append(f\"{param_type}.{wave}\")\n",
    "            if anchor and param_type==\"i\":\n",
    "                Vs[f\"{param_type}.{wave}\"] = 0\n",
    "                Vs[f\"fix_{param_type}.{wave}\"]=True\n",
    "            else:\n",
    "                Vs[f\"{param_type}.{wave}\"] = random.uniform(-200,200)\n",
    "    #             Vs[f\"limit_{param_type}.{wave}\"] = [-500, 1500.]\n",
    "    #             Vs[f\"error_{param_type}.{wave}\"] = .1\n",
    "    return initial, Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveset=[\n",
    "    # S-waves\n",
    "    [\"1.0.0\",True],\n",
    "    [\"-1.0.0\",True],\n",
    "    \n",
    "    # D-waves\n",
    "    [\"1.2.0\",False],\n",
    "    [\"1.2.1\",False],\n",
    "    [\"1.2.2\",False],\n",
    "    [\"-1.2.-1\",False],\n",
    "    [\"-1.2.0\",False],\n",
    "    [\"-1.2.1\",False],\n",
    "    \n",
    "    # P-waves\n",
    "    [\"1.1.0\",False],\n",
    "    [\"1.1.1\",False],\n",
    "    [\"-1.1.0\",False],\n",
    "    [\"-1.1.1\",False]\n",
    "]\n",
    "\n",
    "initial,Vs = setupWave(waveset)\n",
    "    \n",
    "with open(f'{outputpath}/wave_set.csv', 'w') as file:\n",
    "    for key in Vs.keys():\n",
    "        file.write(f\"{key},{Vs[key]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define amplitude and phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vs = {\"errordef\": 1}\n",
    "\n",
    "# initial = []\n",
    "# for param_type in [\"a\", \"p\"]: #try to fit magnitude and angle\n",
    "#     initial.append(f\"{param_type}.1.0.0\")\n",
    "#     initial.append(f\"{param_type}.1.1.0\")\n",
    "#     initial.append(f\"{param_type}.1.1.1\")\n",
    "#     initial.append(f\"{param_type}.1.2.0\")\n",
    "#     initial.append(f\"{param_type}.1.2.1\")\n",
    "#     initial.append(f\"{param_type}.1.2.2\")\n",
    "# #     initial.append(f\"{param_type}.1.2.-1\")\n",
    "# #     initial.append(f\"{param_type}.1.2.-2\")\n",
    "    \n",
    "# Vs[f\"a.1.0.0\"] = 10\n",
    "# Vs[f\"limit_a.1.0.0\"] = [0, 1500.]\n",
    "# Vs[f\"error_a.1.0.0\"] = .1\n",
    "\n",
    "# Vs[f\"a.1.1.0\"] = 10\n",
    "# Vs[f\"limit_a.1.1.0\"] = [0, 1500.]\n",
    "# Vs[f\"error_a.1.1.0\"] = .1\n",
    "\n",
    "# Vs[f\"a.1.1.1\"] = 10\n",
    "# Vs[f\"limit_a.1.1.1\"] = [0, 1500.]\n",
    "# Vs[f\"error_a.1.1.1\"] = .1\n",
    "\n",
    "# Vs[f\"a.1.2.0\"] = 10\n",
    "# Vs[f\"limit_a.1.2.0\"] = [0, 1500.]\n",
    "# Vs[f\"error_a.1.2.0\"] = .1\n",
    "\n",
    "# Vs[f\"a.1.2.1\"] = 10\n",
    "# Vs[f\"limit_a.1.2.1\"] = [0, 1500.]\n",
    "# Vs[f\"error_a.1.2.1\"] = .1\n",
    "\n",
    "# Vs[f\"a.1.2.2\"] = 10\n",
    "# Vs[f\"limit_a.1.2.2\"] = [0, 1500.]\n",
    "# Vs[f\"error_a.1.2.2\"] = .1 \n",
    "\n",
    "# # Vs[f\"a.1.2.-1\"] = 10\n",
    "# # Vs[f\"limit_a.1.2.-1\"] = [0, 1500.]\n",
    "# # Vs[f\"error_a.1.2.-1\"] = .1\n",
    "\n",
    "# # Vs[f\"a.1.2.-2\"] = 10\n",
    "# # Vs[f\"limit_a.1.2.-2\"] = [0, 1500.]\n",
    "# # Vs[f\"error_a.1.2.-2\"] = .1\n",
    "\n",
    "# Vs[f\"p.1.0.0\"] = 0\n",
    "# Vs[f\"limit_p.1.0.0\"] = [0, 2*npy.pi]\n",
    "# Vs[f\"error_p.1.0.0\"] = .01\n",
    "# Vs[f\"fix_p.1.0.0\"] = True\n",
    "\n",
    "# Vs[f\"p.1.1.0\"] = 0\n",
    "# Vs[f\"limit_p.1.1.0\"] = [0, 2*npy.pi]\n",
    "# Vs[f\"error_p.1.1.0\"] = .01\n",
    "\n",
    "# Vs[f\"p.1.1.1\"] = 0\n",
    "# Vs[f\"limit_p.1.1.1\"] = [0, 2*npy.pi]\n",
    "# Vs[f\"error_p.1.1.1\"] = .01\n",
    "\n",
    "# Vs[f\"p.1.2.0\"] = 0\n",
    "# Vs[f\"limit_p.1.2.0\"] = [0, 2*npy.pi]\n",
    "# Vs[f\"error_p.1.2.0\"] = .01\n",
    "\n",
    "# Vs[f\"p.1.2.1\"] = 0\n",
    "# Vs[f\"limit_p.1.2.1\"] = [0, 2*npy.pi]\n",
    "# Vs[f\"error_p.1.2.1\"] = .01\n",
    "\n",
    "# Vs[f\"p.1.2.2\"] = 0\n",
    "# Vs[f\"limit_p.1.2.2\"] = [0, 2*npy.pi]\n",
    "# Vs[f\"error_p.1.2.2\"] = .01 \n",
    "\n",
    "# # Vs[f\"p.1.2.-1\"] = 0\n",
    "# # Vs[f\"limit_p.1.2.-1\"] = [0, 2*npy.pi]\n",
    "# # Vs[f\"error_p.1.2.-1\"] = .01\n",
    "\n",
    "# # Vs[f\"p.1.2.-2\"] = 0\n",
    "# # Vs[f\"limit_p.1.2.-2\"] = [0, 2*npy.pi]\n",
    "# # Vs[f\"error_p.1.2.-2\"] = .01\n",
    "    \n",
    "\n",
    "# print(Vs)\n",
    "\n",
    "# with open(f'{outputpath}/wave_set.csv', 'w') as file:\n",
    "#     for key in Vs.keys():\n",
    "#         file.write(f\"{key},{Vs[key]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AmplitudeJPACfit\n",
    "amplitude = AmplitudeJPACfit.FitAmplitude(initial)\n",
    "# import AmplitudeJPACfitAngles\n",
    "# amplitude = AmplitudeJPACfitAngles.FitAmplitude(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning of the data/monte-carlo and __define amplitude (function) to fit__\n",
    "> Here the user defines number of bins, variable to be binned and range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define number of bins \n",
    "nbins = 30 #45\n",
    "mmin=0.8 #0.7\n",
    "mmax=2.0 #2.5\n",
    "binsda = pwa.bin_by_range(data, \"mass\", nbins, mmin, mmax)\n",
    "binsmc = pwa.bin_by_range(mc, \"mass\", nbins, mmin, mmax)\n",
    "binsda_w = pwa.bin_by_range(data_weight, \"mass\", nbins, mmin, mmax)\n",
    "binsmc_w = pwa.bin_by_range(mc_weight, \"mass\", nbins, mmin, mmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(binsda)):\n",
    "    print(\"bin{0} (data, mc): ({1},{2})\".format(i,len(binsda[i]),len(binsmc[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot events per bin for visualisation and to make sure there are enough events per bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "### DATA\n",
    "counts=[]\n",
    "centers=[]\n",
    "for ibin,bin in enumerate(binsda):\n",
    "    counts.append(npy.sum(binsda_w[ibin][\"weightASBS\"]))\n",
    "    centers.append(npy.mean(bin[\"mass\"]))\n",
    "\n",
    "# Add yerr to argment list when we have errors\n",
    "ax[0].step(centers,counts,label='data')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "### FLAT\n",
    "counts=[]\n",
    "centers=[]\n",
    "for ibin,bin in enumerate(binsmc):\n",
    "    counts.append(npy.sum(binsmc_w[ibin][\"weightASBS\"]))\n",
    "    centers.append(npy.mean(bin[\"mass\"]))\n",
    "\n",
    "# Add yerr to argment list when we have errors\n",
    "ax[1].step(centers,counts,label='data')\n",
    "ax[1].set_ylim(bottom=0)\n",
    "ax[1].legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "intensities = []\n",
    "for the_bin in binsda:\n",
    "    amplitude.setup(the_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting with Minuit and Extended LogLikelihood\n",
    "> Look at other possibilities through pypwa (use the ?pwa command\\\n",
    "> or see https://pypwa.jlab.org or https://github.com/JeffersonLab/PyPWA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawnFit(index):\n",
    "    #############\n",
    "    # Arguments to PyPWA's minuit\n",
    "    # https://github.com/JeffersonLab/PyPWA/blob/342026d5ea9ac37c437081c495142269cbcbe112/PyPWA/libs/fit/minuit.py\n",
    "    #############\n",
    "    # parameters : List[str]\n",
    "    #     The names of the parameters for iminuit to use\n",
    "    # settings : Dict[str, Any]\n",
    "    #     The settings to be passed to iminuit. Look into the documentation\n",
    "    #     for iminuit for specifics\n",
    "    # likelihood : Likelihood object from likelihoods or single function\n",
    "    # set_up : float\n",
    "    #     Set to 1 for log-likelihoods, or .5 for Chi-Squared\n",
    "    # strategy : int\n",
    "    #     Fitting strategy. Defaults to 1. 0 is slowest, 2 is fastest/\n",
    "    # num_of_calls : int\n",
    "    #     A suggested max number of calls to minuit. This may or may not\n",
    "    #     be respected.\n",
    "\n",
    "    # MINUIT itself contains arguments. i.e.\n",
    "    # to get the functions minimum we can use fval on the optimizer:\n",
    "    # https://iminuit.readthedocs.io/en/stable/reference.html\n",
    "    \n",
    "    ##############\n",
    "    # Likelihood arguments\n",
    "    # https://github.com/JeffersonLab/PyPWA/blob/342026d5ea9ac37c437081c495142269cbcbe112/PyPWA/libs/fit/likelihoods.py\n",
    "    ##############\n",
    "    \n",
    "    start=time.time() \n",
    "    results[index]=[]\n",
    "    for ifit in range(nfits):\n",
    "        initial,Vs = setupWave(waveset)\n",
    "        with pwa.LogLikelihood(\n",
    "            amplitude, binsda[index], binsmc[index], quality_factor=binsda_w[index][\"weightASBS\"],\n",
    "            quality_factor_mc=binsmc_w[index][\"weightASBS\"],\n",
    "            #generated_length=len(binsmc_w[index][\"weightASBS\"]),\n",
    "            num_of_processes=1) as Likelihood:\n",
    "            results[index].append(pwa.minuit(initial, Vs, Likelihood, 1, 1, 5000))\n",
    "    end=time.time()\n",
    "    print(f\"Fit #{index} done in {end-start}s\")\n",
    "\n",
    "print(\"Starting pool of workers...\")\n",
    "globalStart = time.time()\n",
    "results = {}\n",
    "nfits=20 # fits per bin\n",
    "nthreads=25\n",
    "assert nthreads <= nbins\n",
    "\n",
    "with ThreadPool(nthreads) as p:\n",
    "    p.map(spawnFit,range(len(binsda)))\n",
    "globalEnd = time.time()\n",
    "\n",
    "# sort the dictionary of minimizers by the keys (bin number)\n",
    "results=list(collections.OrderedDict(sorted(results.items())).values())\n",
    "\n",
    "print(\"Elapsed time: {}\".format(globalEnd-globalStart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDict={}\n",
    "countbins=0\n",
    "for ibin in range(len(results)):\n",
    "    countbins+=1\n",
    "    resultsDict[f\"bin{ibin}\"]={}\n",
    "    for ifit,result in enumerate(results[ibin]):\n",
    "        resultsDict[f\"bin{ibin}\"][f\"fit{ifit}\"]=[\n",
    "            [result.fmin.is_valid,result.fmin.fval],\n",
    "            results[ibin][ifit].np_values(),\n",
    "            results[ibin][ifit].np_errors()\n",
    "        ]\n",
    "print(f\"Total bins: {countbins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of bins: {len(resultsDict.keys())}\")\n",
    "for bin_key in resultsDict.keys():\n",
    "    print(f\"number of fits in {bin_key}: {len(resultsDict[bin_key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fval_idxs=[]\n",
    "all_invalid_fits_idxs=[]\n",
    "for ibin,bin_key in enumerate(resultsDict.keys()):\n",
    "    print(bin_key)\n",
    "    fval_min=npy.inf\n",
    "    fval_idx=0\n",
    "    n_valid=0\n",
    "    for ifit,fit_key in enumerate(resultsDict[bin_key].keys()):\n",
    "        fval=resultsDict[bin_key][fit_key][0][1]\n",
    "        status=resultsDict[bin_key][fit_key][0][0]\n",
    "        status=\"valid\" if status else \"invalid\"\n",
    "        print(f\"  ({ifit})({status})fval: {fval}\")\n",
    "        if fval < fval_min:\n",
    "            fval_min=fval\n",
    "            fval_idx=ifit\n",
    "        if status==\"valid\":\n",
    "            n_valid+=1\n",
    "    if n_valid==0:\n",
    "        all_invalid_fits_idxs.append(ibin)\n",
    "    print(f\"  --- Best fval={fval_min} at idx={fval_idx}\")\n",
    "    print(f\"  --- # valid status={n_valid}\")\n",
    "    fval_idxs.append(fval_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsBest=[]\n",
    "resultsBestErrs=[]\n",
    "resultsBest_optimizer=[]\n",
    "for binNum,bin_key in enumerate(resultsDict.keys()):\n",
    "    fval_idx=fval_idxs[binNum]\n",
    "    resultsBest.append(resultsDict[bin_key][f\"fit{fval_idx}\"][1])\n",
    "    resultsBestErrs.append(resultsDict[bin_key][f\"fit{fval_idx}\"][2])\n",
    "    resultsBest_optimizer.append(results[binNum][fval_idx])\n",
    "resultsBest=npy.array(resultsBest)\n",
    "resultsBestErrs=npy.array(resultsBestErrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = 0\n",
    "for index, result in enumerate(resultsBest_optimizer):\n",
    "    display(f\"Bin #{index}:\")\n",
    "    display(result.get_fmin())\n",
    "    try:\n",
    "        display(result.get_param_states())\n",
    "    except:\n",
    "        failed += 1\n",
    "        pass\n",
    "\n",
    "display(f\"{(failed / len(results)) * 100}% ({failed})Failed. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having trouble pickling the fit results. Will just save the important values and reconstruct the necessary outputs...\n",
    "resultsData={ \"best\":resultsBest, \"best_err\": resultsBestErrs, \"data\":resultsDict , \"parameters\":results[0][0].parameters }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing minuit fit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(resultsData,open(\"minuit_results.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_values = pandas.DataFrame(resultsData[\"best\"], columns=resultsData[\"parameters\"])\n",
    "final_values.to_csv(f\"{outputpath}/minuit.csv\", index=False)\n",
    "\n",
    "final_errs = pandas.DataFrame(resultsData[\"best_err\"], columns=resultsData[\"parameters\"])\n",
    "final_errs.to_csv(f\"{outputpath}/minuit_err.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading minuit fit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsData=pickle.load(open(\"minuit_results.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results of the fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Checking the waves used (and filling waves variable for later use)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waves = amplitude.make_elm(resultsData[\"parameters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Filling strings with wave names (for plotting)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = npy.empty(len(waves),dtype=\"int\")\n",
    "string = npy.empty(len(waves),dtype=\"U5\")\n",
    "for index, w in waves.iterrows():\n",
    "    #print(w[\"l\"])\n",
    "    string[index]= \"{}{}{}\".format(w[\"e\"],w[\"l\"],w[\"m\"])\n",
    "    wave[index]=w.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Getting the bin mass values and number of events in datasample for those bins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmass=[]\n",
    "mcounts=[]\n",
    "for index, bin in enumerate(binsda):\n",
    "    if len(bin)==0:\n",
    "        bmass.append(0.)\n",
    "        mcounts.append(0.)\n",
    "    else:\n",
    "        bmass.append(npy.average(bin[\"mass\"]))\n",
    "        mcounts.append(npy.sum(binsda_w[index][\"weightASBS\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the expected number of events in a mass bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nExp = npy.empty(len(binsmc))\n",
    "for index, the_bin, result in zip(range(len(binsmc)), binsmc, resultsData[\"best\"]):\n",
    "    amp=amplitude\n",
    "    amp.setup(the_bin)\n",
    "    resultMap={resultsData[\"parameters\"][i]:result[i] for i in range(resultsData[\"best\"].shape[1])}\n",
    "    total_nExp[index] = npy.average(binsmc_w[index][\"weightASBS\"]*amp.calculate(resultMap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot expected number of events vs mass and data vs mass (both should agree if fitting worked)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mni = npy.empty(len(total_nExp), dtype=[(\"mass\", float), (\"int\", float)])\n",
    "mda = npy.empty(len(mcounts), dtype=[(\"mass\", float), (\"intd\", float)])\n",
    "mni[\"mass\"] = bmass\n",
    "mni[\"int\"] = total_nExp\n",
    "mni = pandas.DataFrame(mni)\n",
    "counts, bin_edges = npy.histogram(mni[\"mass\"], nbins, weights=mni[\"int\"])\n",
    "mda[\"mass\"] = bmass\n",
    "mda[\"intd\"] = mcounts\n",
    "mda = pandas.DataFrame(mda)\n",
    "dcounts, bin_edges = npy.histogram(mda[\"mass\"], nbins, weights=mda[\"intd\"])\n",
    "#dcounts, bin_edges = npy.histogram(bmass, nbins, weights=total_nExp)\n",
    "centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Add yerr to argment list when we have errors\n",
    "yerr = npy.empty(nbins)\n",
    "yerr = npy.sqrt(counts)\n",
    "myerr = npy.empty(nbins)\n",
    "myerr = npy.sqrt(dcounts)\n",
    "\n",
    "#plt.errorbar(centers,counts, yerr, fmt=\"s\",linestyle=\"dashed\",markersize='10',label=\"nExp\")\n",
    "width=(centers[1]-centers[0])/2\n",
    "plt.step(centers+width,mcounts,label=\"Data\",linestyle='-',c='gray')\n",
    "plt.scatter(centers,total_nExp,label=\"Fitted\",linestyle='-',s=32,color='steelblue',alpha=1)\n",
    "plt.legend(loc='upper right',prop={\"size\":20})\n",
    "plt.xlabel(r\"$M(\\eta\\pi)$\",size=20)\n",
    "plt.ylabel(\"Intensity\",size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate initial intensities (in case we need to check them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = []\n",
    "for the_bin in binsda:\n",
    "    amp.setup(the_bin)\n",
    "    intensities.append(amp.calculate(Vs))\n",
    "intesities=pandas.DataFrame(intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the expected number of events for each wave (vs mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Calculate the wave content\n",
    "####################  \n",
    "\n",
    "total_nExp = npy.empty(len(binsmc))\n",
    "for index, the_bin, result in zip(range(len(binsmc)), binsmc, resultsData[\"best\"]):\n",
    "    amp=amplitude\n",
    "    amp.setup(the_bin)\n",
    "    resultMap={resultsData[\"parameters\"][i]:result[i] for i in range(resultsData[\"best\"].shape[1])}\n",
    "    total_nExp[index] = npy.average(binsmc_w[index][\"weightASBS\"]*amp.calculate(resultMap))\n",
    "\n",
    "\n",
    "wave_nExp = npy.empty([len(waves),len(binsmc)],npy.longdouble)\n",
    "for index, the_bin, result in zip(range(len(binsmc)), binsmc, resultsData[\"best\"]):\n",
    "    amp.setup(the_bin)\n",
    "    resultMap={resultsData[\"parameters\"][i]:result[i] for i in range(resultsData[\"best\"].shape[1])}\n",
    "    for i in range(len(waves)):\n",
    "        wave_nExp[i][index] = npy.average(binsmc_w[index][\"weightASBS\"]*amp.calculate_wave(resultMap,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected number of events vs mass for each wave in a same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for the unique LM waves. We will plot each LM wave on a different axis\n",
    "#   with different colors for the + and - reflectivity waves\n",
    "lms=[]\n",
    "for i,w in waves.iterrows():\n",
    "    lm=f\"{w['l']}_{w['m']}\"\n",
    "    lms.append(lm)\n",
    "lms=npy.unique(npy.array(lms))\n",
    "lms=[lm.split(\"_\") for lm in lms]\n",
    "lms0=[lm for lm in lms if lm[0]=='0']\n",
    "lms1=[lm for lm in lms if lm[0]=='1']\n",
    "lms2=[lm for lm in lms if lm[0]=='2']\n",
    "lms=lms0+lms2+lms1\n",
    "lms=npy.array(lms)\n",
    "\n",
    "# Need to track which array element holds which wave\n",
    "mapWaves={wave:i for i,wave in enumerate(string)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Make the final plot\n",
    "######################\n",
    "\n",
    "fig,ax=plt.subplots(3,3,figsize=(14,14),sharey=True)\n",
    "ax=ax.flatten()\n",
    "\n",
    "for iax,lm in enumerate(lms):\n",
    "    ax[iax].step(centers+width,mcounts,label=\"Data\",linestyle='-',c='gray')\n",
    "    for ref in [\"1\",\"-1\"]:\n",
    "        if ref+lm[0]+lm[1] in mapWaves:\n",
    "            i = mapWaves[ref+lm[0]+lm[1]]\n",
    "            color = \"indianred\" if ref==\"1\" else \"steelblue\"\n",
    "            ax[iax].scatter(centers,wave_nExp[i],label=string[i],c=color,s=12)\n",
    "    legend=ax[iax].legend(loc='upper right', title=\"ε L M\",prop={\"size\":12})\n",
    "    legend.get_title().set_fontsize('20')\n",
    "    ax[iax].set_xlabel(r\"$M(\\pi^0\\eta) GeV$\",size=20)\n",
    "    ylims=ax[iax].set_ylim(bottom=0)\n",
    "    \n",
    "for iax in [0,3,6]:\n",
    "    ax[iax].set_ylabel(\"Intensity / 40 MeV\",size=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read fitted amplitudes from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minuitresults = pandas.read_csv(f\"{outputpath}/minuit.csv\")\n",
    "\n",
    "## Setup fit config for amp / phase\n",
    "import AmplitudeJPACfitAngles\n",
    "\n",
    "\n",
    "def convertToAP(samples):\n",
    "    '''\n",
    "    Convert samples from real/imaginary to amplitude/phase\n",
    "    '''\n",
    "    apsamples=npy.zeros(samples.shape)\n",
    "\n",
    "    waves=[amp[2:] for amp in initial if amp[0]==\"r\"]\n",
    "\n",
    "    for iw,wave in enumerate(waves):\n",
    "        apsamples[...,2*iw]=npy.sqrt(samples[...,2*iw]**2+samples[...,2*iw+1]**2)\n",
    "        apsamples[...,2*iw+1]=npy.arctan2(samples[...,2*iw+1],samples[...,2*iw])\n",
    "        apsamples[...,2*iw+1][npy.abs(apsamples[...,2*iw+1]-3.14159) < 0.00001] = 0.0\n",
    "    return apsamples\n",
    "\n",
    "######################\n",
    "# Define waveset\n",
    "######################\n",
    "apinitial = []\n",
    "for val in initial:\n",
    "    if val.startswith('r'):\n",
    "        apinitial.append(val.replace('r.','a.'))\n",
    "    else:\n",
    "        apinitial.append(val.replace('i.','p.'))\n",
    "    \n",
    "######################\n",
    "# Convert from re/im to amp/phase\n",
    "######################\n",
    "amplitudeAngles = AmplitudeJPACfitAngles.FitAmplitude(apinitial)\n",
    "waves=[amp[2:] for amp in initial if amp[0]==\"a\" or amp[0]==\"i\"]\n",
    "\n",
    "apminuitresults = minuitresults.copy()\n",
    "for i in range(len(apminuitresults)):\n",
    "    for iw,wave in enumerate(waves):\n",
    "        # amp = sqrt(Re^2+Im^2)\n",
    "        apminuitresults.loc[i][2*iw] = npy.sqrt(minuitresults.loc[i][2*iw]**2+minuitresults.loc[i][2*iw+1]**2)\n",
    "        # phase = atan2(Im,Re)\n",
    "        #apminuitresults.loc[i][2*iw+1] = npy.arctan2(minuitresults.loc[i][2*iw+1],minuitresults.loc[i][2*iw])\n",
    "        apminuitresults.loc[i][2*iw+1] = npy.angle(minuitresults.loc[i][2*iw+1]+1j*minuitresults.loc[i][2*iw])\n",
    "\n",
    "apminuitresults.columns = apinitial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "_label=\"ri\" #ap\n",
    "_amplitude=amplitude #amplitudeAngles\n",
    "_results=minuitresults # apminuitresults\n",
    "_initial=initial #apinitial\n",
    "startvals = _results.loc[index].copy()\n",
    "if _label == \"ri\":\n",
    "    for p in range(len(startvals)):\n",
    "        if startvals[p]!=0:\n",
    "            startvals[p] = startvals[p]*(1+npy.random.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Configuration\n",
    "################################\n",
    "numwalker=100\n",
    "numsteps=50*200 # the autocorrelation time seems to be around 150 for our chains\n",
    "ncpu=4\n",
    "nthreads=24\n",
    "\n",
    "_label=\"ri\" #ap\n",
    "_amplitude=amplitude #amplitudeAngles\n",
    "_results=minuitresults # apminuitresults\n",
    "_initial=initial #apinitial\n",
    "################################\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THINNING: (thin)ning is rarely recommended these days. There are correlations betweeen nearby samples inherent in the sample procedure.\n",
    "# We can reduce the correlation by thinning the chain, basically grabbing every other Nth sample. This does reduce the chains length and therefore\n",
    "# the variance of the approximation BUT does gives us better iid samples if that were every needed\n",
    "# https://stats.stackexchange.com/questions/442714/why-does-thinning-work-in-bayesian-inference\n",
    "\n",
    "\n",
    "def spawnMCMC(index):\n",
    "#     with pwa.LogLikelihood(\n",
    "#         amplitude, dbin, binsma[index], generated_length=len(binsma[index]), num_of_processes=ncpu, is_minimizer=False) as Likelihood:\n",
    "    with pwa.LogLikelihood(\n",
    "        _amplitude, binsda[index], binsmc[index], quality_factor=binsda_w[index][\"weightASBS\"],\n",
    "        quality_factor_mc=binsmc_w[index][\"weightASBS\"],\n",
    "        num_of_processes=ncpu, is_minimizer=False) as Likelihood:\n",
    "        print(f\"Find random start parameters for fit #{index}...\")\n",
    "        startpars = npy.zeros((numwalker,len(_initial)))\n",
    "        for i in range(numwalker):\n",
    "            a=0\n",
    "            while True:\n",
    "                a+=1\n",
    "                \n",
    "                #add some randomization here for moves to work better\n",
    "                startvals = _results.loc[index].copy()\n",
    "                if _label == \"ri\":\n",
    "                    for p in range(len(startvals)):\n",
    "                        if startvals[p]!=0:\n",
    "                            startvals[p] = startvals[p]*(1+npy.random.uniform(-0.1,0.1))#-0.5,0.5))\n",
    "                else:\n",
    "                    for iw in range(0,len(waves)):\n",
    "                        startvals[2*iw] = startvals[2*iw]*(1+npy.random.uniform(-0.1,0.1))#-0.5,0.5))\n",
    "                        #initialise the phases with an overall offset to go round on the circle. \n",
    "                        #We will get the remainder first and then scale\n",
    "                        startvals[2*iw+1] = (startvals[2*iw+1] % (2*npy.pi))*(1+npy.random.uniform(-0.1,0.1))#-0.5,0.5))\n",
    "                    startvals[1]=0 #fix phase for s wave\n",
    "                    startvals[3]=0 #fix phase for s wave\n",
    "\n",
    "                nll = Likelihood(startvals.to_dict())\n",
    "                if not npy.any(npy.isnan(nll)):\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "            # print(f\"After {a} steps found nll={nll}.\") #debugging\n",
    "            startpars[i] = list(startvals)\n",
    "\n",
    "        cov =  1 #npy.array([0.002,0.002,0.002,0.005,0.005,0.005,0.005,0.005,0.005,1E5])\n",
    "        if _label == \"ri\":  \n",
    "            parlimits = [(-1000,1000),(0,0),(-1000,1000),(0,0)] # set the s-waves to be real\n",
    "            for i in range(4,len(_initial)):\n",
    "                parlimits.append((-1000,1000))\n",
    "        else:\n",
    "            parlimits=[[] for i in range(2*len(waves))]\n",
    "            for iw in range(0,len(waves)):\n",
    "                parlimits[2*iw]=(0,1500) \n",
    "                if iw<2: # fix the s-waves\n",
    "                    parlimits[2*iw+1]=(0,0)\n",
    "                else:\n",
    "                    parlimits[2*iw+1]=(0,2*npy.pi) \n",
    "\n",
    "        print(f\"Start chain for bin {index} ...\")\n",
    "        # https://emcee.readthedocs.io/en/stable/user/moves/#moves-user\n",
    "        # Sample from a set of moves. Moves are broken up into two paths {RedBlueMove, MHMove} which are parallel and non-parallel I think. These are two\n",
    "        #         base abstract classes that other moves inherit from. GaussianMove is a MHMove with a Gaussian proposal function\n",
    "        # StretchMove - Moving the walker based on a complementary walker\n",
    "        # DE = Differential evolution, Multiple chains are run in parallel. The current move depends on two other chains. \n",
    "        #           Successful sampling requires at least 2*d chains where d is the dimensionality of the posterior\n",
    "        # DESnooker applies a snooker move which allows for shorter chains. The move respects past states of other chains and thus allows for parallelization\n",
    "        optimizer = pwa.mcmc(\n",
    "                        _initial,\n",
    "                        likelihood=Likelihood,\n",
    "                        nsteps=numsteps,\n",
    "                        startpars=startpars,\n",
    "                        parlimits=parlimits,\n",
    "                        nwalker=numwalker,\n",
    "                        #emceemoves=[(emcee.moves.StretchMove(), 0.5), (emcee.moves.DEMove(), 0.3), (emcee.moves.DESnookerMove(), 0.2)])\n",
    "                        emceemoves=[(emcee.moves.StretchMove(), 0.9*0.1), (emcee.moves.DEMove(), 0.9*0.9), (emcee.moves.DESnookerMove(), 0.1)])\n",
    "#                              emceemoves=emcee.moves.StretchMove())\n",
    "#                              emceemoves=emcee.moves.GaussianMove(cov,'vector'))\n",
    "\n",
    "\n",
    "    _resultsMCMC[index]=optimizer\n",
    "    \n",
    "    acceptanceFraction=npy.mean(optimizer.acceptance_fraction)\n",
    "    print(\"Mean acceptance fraction: {0:.3f}\".format(npy.mean(optimizer.acceptance_fraction)))\n",
    "    acceptanceFractions[index]=acceptanceFraction\n",
    "    try:\n",
    "        autocorrelationTime=npy.mean(optimizer.get_autocorr_time()) # will raise an error if autocorrelation too big, can quite=True it\n",
    "        print(\"Mean autocorrelation time: {0:.3f} steps\".format(autocorrelationTime))\n",
    "        autocorrelationTimes[index]=autocorrelationTime\n",
    "    except:\n",
    "        print(\"Couldn't get autocorrelation time.\")\n",
    "        autocorrelationTimes[index] = -1\n",
    "    \n",
    "#     break #only one bin\n",
    "\n",
    "\n",
    "print(\"Starting pool of workers for mcmc...\")\n",
    "globalStart = time.time()\n",
    "_resultsMCMC = {}\n",
    "acceptanceFractions = {}\n",
    "autocorrelationTimes = {}\n",
    "\n",
    "with ThreadPool(nthreads) as p:\n",
    "    p.map(spawnMCMC,range(len(binsda)))\n",
    "globalEnd = time.time()\n",
    "\n",
    "# sort the dictionary of minimizers by the keys (bin number)\n",
    "_resultsMCMC=list(collections.OrderedDict(sorted(_resultsMCMC.items())).values())\n",
    "acceptanceFractions=list(collections.OrderedDict(sorted(acceptanceFractions.items())).values())\n",
    "autocorrelationTimes=list(collections.OrderedDict(sorted(autocorrelationTimes.items())).values())\n",
    "\n",
    "print(\"Elapsed time: {}\".format(globalEnd-globalStart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pkl = {}\n",
    "results_pkl[\"chain\"] = [result.get_chain(discard=0) for result in _resultsMCMC]\n",
    "#results_pkl[\"autocorr\"] = [result.get_autocorr_time() for result in _resultsMCMC]\n",
    "results_pkl[\"acceptance_fraction\"] = [result.acceptance_fraction for result in _resultsMCMC]\n",
    "\n",
    "pickle.dump(results_pkl, open(\"resultsMCMC.pkl\",\"wb\"))\n",
    "\n",
    "# for i in range(len(binsda)):\n",
    "#     samples = resultsMCMC[i].get_chain(discard=0)\n",
    "#     pwa.write(f\"{outputpath}/mcmc_mix_{numwalker}_{numsteps}_bin{i}.npy\",samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read MCMC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbins=45\n",
    "# samplesList=[]\n",
    "# for i in range(nbins):\n",
    "#     samples=npy.load(f\"{outputpath}/mcmc_mix_{numwalker}_{numsteps}_bin{i}.npy\")\n",
    "#     samplesList.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resultsMCMC = pickle.load(open(\"resultsMCMC.pkl\",\"rb\"))\n",
    "resultsMCMC = pickle.load(open(\"resultsMCMC_15ksteps_100walkers_10percRnd.pkl\",\"rb\"))\n",
    "\n",
    "samplesList=resultsMCMC[\"chain\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC diagnostic plots - autocorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real and imaginary parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two are equivalent\n",
    "# NOTE THAT integrated_time ONLY WORKS IF IN ORDER [STEPS, WALKERS, PARAMS]\n",
    "# resultsMCMC[0].get_autocorr_time()\n",
    "# emcee.autocorr.integrated_time(resultsMCMC[0].get_chain(discard=0))\n",
    "\n",
    "binNum=0\n",
    "\n",
    "minstep=100\n",
    "maxstep=resultsMCMC[\"chain\"][0].shape[0] #15000\n",
    "widthstep=100\n",
    "steps=range(minstep,maxstep,widthstep)\n",
    "\n",
    "autocorrs=[]\n",
    "for step in steps:\n",
    "    #autocorrs.append(emcee.autocorr.integrated_time(resultsMCMC[binNum].get_chain(discard=0)[:step,:,:],quiet=True))\n",
    "    autocorrs.append(emcee.autocorr.integrated_time(resultsMCMC[\"chain\"][binNum][:step,:,:],quiet=True))\n",
    "    \n",
    "autocorrs=npy.stack(autocorrs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2,figsize=(16,12))\n",
    "\n",
    "for iparam in range(autocorrs.shape[0]):\n",
    "    _=ax[0,0].plot(steps,autocorrs[iparam])\n",
    "ax[0,0].set_xlabel(\"Step\")\n",
    "ax[0,0].set_ylabel(\"Integrated Autocorrelation Time\")\n",
    "ax[0,0].set_title(f\"Mass Bin: {binNum}\",size=30)\n",
    "\n",
    "iparam=0\n",
    "for iwalker in range(5):\n",
    "    _=ax[0,1].plot(resultsMCMC[\"chain\"][binNum][:,iwalker,iparam],label=\"walker \"+str(iwalker))\n",
    "ax[0,1].set_xlabel(\"step\")\n",
    "ax[0,1].set_title(f\"parameter: {initial[iparam]}\",size=30)\n",
    "ax[0,1].legend(prop={\"size\":16})\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "ax[1,0].hist(results_pkl[\"acceptance_fraction\"],bins=25,stacked=True,color=list(map(cmap,npy.linspace(0,1,45))))\n",
    "ax[1,0].set_xlabel(\"acceptance probability\")\n",
    "ax[1,0].set_title(\"Stacked over mass bins\",size=30)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude and phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two are equivalent\n",
    "# resultsMCMC[0].get_autocorr_time()\n",
    "# emcee.autocorr.integrated_time(resultsMCMC[0].get_chain(discard=0))\n",
    "\n",
    "binNum=0\n",
    "minstep=100\n",
    "maxstep=resultsMCMC[\"chain\"][0].shape[0] #15000\n",
    "widthstep=100\n",
    "steps=range(minstep,maxstep,widthstep)\n",
    "\n",
    "apsamples=convertToAP(resultsMCMC[\"chain\"][binNum])\n",
    "\n",
    "autocorrs_ap=[]\n",
    "for step in steps:\n",
    "    #autocorrs[iwalker].append(emcee.autocorr.integrated_time(flatSamplesList[:step,0],quiet=True)[0])\n",
    "    autocorrs_ap.append(emcee.autocorr.integrated_time(apsamples[:step,:,:],quiet=True))\n",
    "    \n",
    "autocorrs_ap=npy.stack(autocorrs_ap,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2,figsize=(16,12))\n",
    "\n",
    "for iparam in range(autocorrs_ap.shape[0]):\n",
    "    _=ax[0,0].plot(steps,autocorrs_ap[iparam])\n",
    "ax[0,0].set_xlabel(\"Step\")\n",
    "ax[0,0].set_ylabel(\"Integrated Autocorrelation Time\")\n",
    "ax[0,0].set_title(f\"Mass Bin: {binNum}\",size=30)\n",
    "\n",
    "iparam=0\n",
    "for iwalker in range(5):\n",
    "    _=ax[0,1].plot(apsamples[:,iwalker,iparam],label=\"walker \"+str(iwalker))\n",
    "ax[0,1].set_xlabel(\"step\")\n",
    "ax[0,1].set_title(f\"parameter: {apinitial[iparam]}\",size=30)\n",
    "ax[0,1].legend(prop={\"size\":16})\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "ax[1,0].hist(results_pkl[\"acceptance_fraction\"],bins=25,stacked=True,color=list(map(cmap,npy.linspace(0,1,45))))\n",
    "ax[1,0].set_xlabel(\"acceptance probability\")\n",
    "ax[1,0].set_title(\"Stacked over mass bins\",size=30)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare MAP and MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Gather the MAP solutions for all parameters for all bins\n",
    "###############################\n",
    "\n",
    "# Some configuration\n",
    "samplesList=resultsMCMC[\"chain\"]\n",
    "nparams=resultsMCMC[\"chain\"][0].shape[2]\n",
    "burnin=5000 #10000\n",
    "maxsteps=10000\n",
    "verbose=False\n",
    "\n",
    "def getMAPsolutions(binNum):\n",
    "    lowMass=0.7+0.04*binNum\n",
    "    uppMass=0.7+0.04*(binNum+1)\n",
    "    print(\"(Thread {0:.0f})Mass range: {1:.2f},{2:.2f}\".format(binNum,lowMass,uppMass))\n",
    "    flatsample=samplesList[binNum][burnin:maxsteps,...].reshape((maxsteps-burnin)*numwalker,24)\n",
    "    flatsample=convertToAP(flatsample)\n",
    "\n",
    "    for idx in range(nparams):\n",
    "        wave=flatsample[:,idx]\n",
    "\n",
    "        X = npy.histogram(wave,bins=100)\n",
    "        centers=X[1][:-1]+(X[1][1]-X[1][0])/2\n",
    "        \n",
    "        if wave.std() != 0: # bandwidth must be > 0. wave.std not always guaranteed. i.e. if the wave was fixed to be real\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=wave.std()/4).fit(wave.reshape(-1,1))\n",
    "            density = npy.exp(kde.score_samples(centers.reshape(-1,1)))\n",
    "            map_solution=centers[npy.argmax(density)]\n",
    "        else:\n",
    "            density=X[0]/sum(X[0])\n",
    "            map_solution=0\n",
    "        \n",
    "        # Fill dictionaries\n",
    "        map_solutions[idx][binNum]=map_solution\n",
    "        densities[idx][binNum]=density\n",
    "        centerses[idx][binNum]=centers\n",
    "        \n",
    "\n",
    "globalStart = time.time()\n",
    "\n",
    "# Pass getMAPsolutions to all the threads\n",
    "map_solutions=[{} for idx in range(nparams)]\n",
    "densities=[{} for idx in range(nparams)]\n",
    "centerses=[{} for idx in range(nparams)]\n",
    "nthreads=24\n",
    "with ThreadPool(nthreads) as p:\n",
    "    p.map(getMAPsolutions,range(len(binsda)))\n",
    "    \n",
    "globalEnd = time.time()\n",
    "\n",
    "for i in range(nparams):\n",
    "    map_solutions[i]=list(collections.OrderedDict(sorted(map_solutions[i].items())).values())\n",
    "    densities[i]=list(collections.OrderedDict(sorted(densities[i].items())).values())\n",
    "    centerses[i]=list(collections.OrderedDict(sorted(centerses[i].items())).values())\n",
    "\n",
    "print(\"Elapsed time: {}\".format(globalEnd-globalStart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Plotting specific waves with the following labels attached\n",
    "# Useful to visualize the difference between MAP and MLE\n",
    "###########################################################\n",
    "idx_of_D2_amp=npy.where(npy.array(apinitial)==\"a.1.2.2\")[0][0]\n",
    "idx_of_D2_phase=npy.where(npy.array(apinitial)==\"p.1.2.2\")[0][0]\n",
    "labels=[\"D2 intensity\", \"D2 phase\"]\n",
    "\n",
    "nbins=45\n",
    "nrows=int(npy.ceil(nbins/2))\n",
    "fig,ax=plt.subplots(nrows,4,figsize=(16, 4*nrows))\n",
    "ax=ax.flatten()\n",
    "\n",
    "for binNum in range(nbins):\n",
    "    flatsample=samplesList[binNum][burnin:maxsteps,...].reshape((maxsteps-burnin)*numwalker,24)\n",
    "    flatsample=convertToAP(flatsample)\n",
    "    \n",
    "    lowMass=0.7+0.04*binNum\n",
    "    uppMass=0.7+0.04*(binNum+1)\n",
    "\n",
    "    for i,idx in enumerate([idx_of_D2_amp, idx_of_D2_phase]):\n",
    "        wave=flatsample[:,idx]\n",
    "    \n",
    "        map_solution=[ele[binNum] for ele in map_solutions][idx]\n",
    "        density=densities[idx][binNum]\n",
    "        centers=centerses[idx][binNum]\n",
    "        \n",
    "        _=ax[2*binNum+i].hist(wave,bins=50,density=True,alpha=0.6)\n",
    "        ax[2*binNum+i].plot(centers,density,linewidth=3,c=\"royalblue\")\n",
    "        ax[2*binNum+i].axvline(map_solution,c='indianred',linestyle='--',linewidth=3)\n",
    "        ax[2*binNum+i].axvline(apminuitresults.iloc[binNum,idx],c='darkorange',linestyle=\"--\",linewidth=3)\n",
    "        for fit_key in resultsData[\"data\"][f\"bin{binNum}\"].keys():\n",
    "            alt_fitresults=resultsData[\"data\"][f\"bin{binNum}\"][fit_key]\n",
    "            alt_fitresults_status=alt_fitresults[0][0]\n",
    "            coloralphawidth = [\"black\",0.8,1] if alt_fitresults_status else [\"gray\",0.6,0.5]\n",
    "            alt_fitresults_values=convertToAP(alt_fitresults[1])[idx]\n",
    "            ax[2*binNum+i].axvline(alt_fitresults_values,c=coloralphawidth[0],linestyle=\"--\",alpha=coloralphawidth[1],linewidth=coloralphawidth[2])\n",
    "        ax[2*binNum+i].set_xlabel(labels[i])\n",
    "        ax[2*binNum+i].set_title(r\"{0:0.2f}<Mpi0eta<{0:0.2f}\".format(lowMass,uppMass),size=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles_16=[]\n",
    "percentiles_50=[]\n",
    "percentiles_84=[]\n",
    "for binNum in range(nbins):\n",
    "    print(f\"bin{binNum}\")\n",
    "    flatsample=samplesList[binNum][burnin:maxsteps,...].reshape((maxsteps-burnin)*numwalker,24)\n",
    "    flatsample=convertToAP(flatsample)\n",
    "    percentiles_16.append([])\n",
    "    percentiles_50.append([])\n",
    "    percentiles_84.append([])\n",
    "    for idx in range(nparams):\n",
    "        wave=flatsample[:,idx]\n",
    "        percentile_16, percentile_50, percentile_84 = npy.percentile(wave, [16, 50, 84])\n",
    "        percentiles_16[binNum].append(percentile_16)\n",
    "        percentiles_50[binNum].append(percentile_50)\n",
    "        percentiles_84[binNum].append(percentile_84)\n",
    "        \n",
    "percentiles_16=npy.array(percentiles_16).T\n",
    "percentiles_50=npy.array(percentiles_50).T\n",
    "percentiles_84=npy.array(percentiles_84).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Convert map_solutions array into a list of dictionary format that is usable by amp.calculate_wave\n",
    "####################\n",
    "map_solution_dicts=[]\n",
    "for bin_num in range(nbins):\n",
    "    map_solution_dict={}\n",
    "    for ipar,par in enumerate(resultsData[\"parameters\"]):\n",
    "        map_solution_dict[par] = map_solutions[ipar][bin_num]\n",
    "    map_solution_dicts.append(map_solution_dict)\n",
    "\n",
    "####################\n",
    "# Make calculate the wave content\n",
    "####################\n",
    "waves = amplitude.make_elm(resultsData[\"parameters\"])\n",
    "wave_nExp = npy.empty([len(waves),len(binsmc)],npy.longdouble)\n",
    "for index, the_bin, result in zip(range(len(binsmc)), binsmc, map_solution_dicts):\n",
    "    amp.setup(the_bin)\n",
    "    for i in range(len(waves)):\n",
    "        wave_nExp[i][index] = npy.average(binsmc_w[index][\"weightASBS\"]*amp.calculate_wave(result,i))\n",
    "        \n",
    "######################\n",
    "# Make the final plot\n",
    "######################\n",
    "# look for the unique LM waves. We will plot each LM wave on a different axis\n",
    "#   with different colors for the + and - reflectivity waves\n",
    "lms=[]\n",
    "for i,w in waves.iterrows():\n",
    "    lm=f\"{w['l']}_{w['m']}\"\n",
    "    lms.append(lm)\n",
    "lms=npy.unique(npy.array(lms))\n",
    "lms=[lm.split(\"_\") for lm in lms]\n",
    "lms0=[lm for lm in lms if lm[0]=='0']\n",
    "lms1=[lm for lm in lms if lm[0]=='1']\n",
    "lms2=[lm for lm in lms if lm[0]=='2']\n",
    "lms=lms0+lms2+lms1\n",
    "lms=npy.array(lms)\n",
    "\n",
    "# Need to track which array element holds which wave\n",
    "mapWaves={wave:i for i,wave in enumerate(string)}\n",
    "\n",
    "fig,ax=plt.subplots(3,3,figsize=(14,14),sharey=True)\n",
    "ax=ax.flatten()\n",
    "\n",
    "centers=[]\n",
    "for ibin,bin in enumerate(binsmc):\n",
    "    centers.append(npy.mean(bin[\"mass\"]))\n",
    "for iax,lm in enumerate(lms):\n",
    "    ax[iax].step(centers+width,mcounts,label=\"Data\",linestyle='-',c='gray')\n",
    "    for ref in [\"1\",\"-1\"]:\n",
    "        if ref+lm[0]+lm[1] in mapWaves:\n",
    "            i = mapWaves[ref+lm[0]+lm[1]]\n",
    "            color = \"indianred\" if ref==\"1\" else \"steelblue\"\n",
    "            ax[iax].scatter(centers,wave_nExp[i],label=string[i],c=color,s=12)\n",
    "    legend=ax[iax].legend(loc='upper right', title=\"ε L M\",prop={\"size\":12})\n",
    "    legend.get_title().set_fontsize('20')\n",
    "    ax[iax].set_xlabel(r\"$M(\\pi^0\\eta) GeV$\",size=20)\n",
    "    ylims=ax[iax].set_ylim(bottom=0)\n",
    "    \n",
    "for iax in [0,3,6]:\n",
    "    ax[iax].set_ylabel(\"Intensity / 40 MeV\",size=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Convert map_solutions array into a list of dictionary format that is usable by amp.calculate_wave\n",
    "####################\n",
    "percentile_16_solution_dicts=[]\n",
    "percentile_50_solution_dicts=[]\n",
    "percentile_84_solution_dicts=[]\n",
    "for bin_num in range(nbins):\n",
    "    percentile_16_solution_dict={}\n",
    "    percentile_50_solution_dict={}\n",
    "    percentile_84_solution_dict={}\n",
    "    for ipar,par in enumerate(resultsData[\"parameters\"]):\n",
    "        percentile_16_solution_dict[par] = percentiles_16[ipar][bin_num]\n",
    "        percentile_50_solution_dict[par] = percentiles_50[ipar][bin_num]\n",
    "        percentile_84_solution_dict[par] = percentiles_84[ipar][bin_num]\n",
    "    percentile_16_solution_dicts.append(percentile_16_solution_dict)\n",
    "    percentile_50_solution_dicts.append(percentile_50_solution_dict)\n",
    "    percentile_84_solution_dicts.append(percentile_84_solution_dict)\n",
    "\n",
    "\n",
    "####################\n",
    "# Make calculate the wave content\n",
    "####################\n",
    "waves = amplitude.make_elm(resultsData[\"parameters\"])\n",
    "wave_nExp_50_err = npy.empty([len(waves),len(binsmc)],list)\n",
    "wave_nExp_50 = npy.empty([len(waves),len(binsmc)],npy.longdouble)\n",
    "for index, the_bin, result_16, result_50, result_84 in zip(range(len(binsmc)), binsmc, percentile_16_solution_dicts, percentile_50_solution_dicts, percentile_84_solution_dicts):\n",
    "    amp.setup(the_bin)\n",
    "    for i in range(len(waves)):\n",
    "        wave_nExp_50[i][index] = npy.average(binsmc_w[index][\"weightASBS\"]*amp.calculate_wave(result_50,i))\n",
    "        wave_nExp_16 = npy.average(binsmc_w[index][\"weightASBS\"]*amp.calculate_wave(result_16,i))\n",
    "        wave_nExp_84 = npy.average(binsmc_w[index][\"weightASBS\"]*amp.calculate_wave(result_84,i))\n",
    "        wave_nExp_50_err[i][index] = [abs(wave_nExp_50[i][index]-wave_nExp_16),abs(wave_nExp_50[i][index]-wave_nExp_84)]\n",
    "\n",
    "        \n",
    "######################\n",
    "# Make the final plot\n",
    "######################\n",
    "# look for the unique LM waves. We will plot each LM wave on a different axis\n",
    "#   with different colors for the + and - reflectivity waves\n",
    "lms=[]\n",
    "for i,w in waves.iterrows():\n",
    "    lm=f\"{w['l']}_{w['m']}\"\n",
    "    lms.append(lm)\n",
    "lms=npy.unique(npy.array(lms))\n",
    "lms=[lm.split(\"_\") for lm in lms]\n",
    "lms0=[lm for lm in lms if lm[0]=='0']\n",
    "lms1=[lm for lm in lms if lm[0]=='1']\n",
    "lms2=[lm for lm in lms if lm[0]=='2']\n",
    "lms=lms0+lms2+lms1\n",
    "lms=npy.array(lms)\n",
    "\n",
    "# Need to track which array element holds which wave\n",
    "mapWaves={wave:i for i,wave in enumerate(string)}\n",
    "\n",
    "fig,ax=plt.subplots(3,3,figsize=(14,14),sharey=True)\n",
    "ax=ax.flatten()\n",
    "\n",
    "centers=[]\n",
    "for ibin,bin in enumerate(binsmc):\n",
    "    centers.append(npy.mean(bin[\"mass\"]))\n",
    "for iax,lm in enumerate(lms):\n",
    "    ax[iax].step(centers+width,mcounts,label=\"Data\",linestyle='-',c='gray')\n",
    "    for ref in [\"1\",\"-1\"]:\n",
    "        if ref+lm[0]+lm[1] in mapWaves:\n",
    "            i = mapWaves[ref+lm[0]+lm[1]]\n",
    "            color = \"indianred\" if ref==\"1\" else \"steelblue\"\n",
    "            errs=npy.array([ele for ele in wave_nExp_50_err[i]]).T # need to convert to array first and then transpose it\n",
    "            ax[iax].errorbar(centers,wave_nExp_50[i],yerr=errs,label=string[i],c=color,markersize=3,ls='none',fmt='o')\n",
    "    legend=ax[iax].legend(loc='upper right', title=\"ε L M\",prop={\"size\":12})\n",
    "    legend.get_title().set_fontsize('20')\n",
    "    ax[iax].set_xlabel(r\"$M(\\pi^0\\eta) GeV$\",size=20)\n",
    "    ylims=ax[iax].set_ylim(bottom=0)\n",
    "    \n",
    "for iax in [0,3,6]:\n",
    "    ax[iax].set_ylabel(\"Intensity / 40 MeV\",size=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw chains and histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apminuitresults = minuitresults.copy()\n",
    "for i in range(len(apminuitresults)):\n",
    "    for iw,wave in enumerate(waves):\n",
    "        amp=npy.sqrt(minuitresults.loc[i][2*iw]**2+minuitresults.loc[i][2*iw+1]**2)\n",
    "        phase=npy.arctan2(minuitresults.loc[i][2*iw+1],minuitresults.loc[i][2*iw])\n",
    "        # amp = sqrt(Re^2+Im^2)\n",
    "        apminuitresults.loc[i][2*iw] = amp\n",
    "        # phase = atan2(Im,Re)\n",
    "        apminuitresults.loc[i][2*iw+1] = phase\n",
    "\n",
    "apminuitresults.columns = apinitial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawnPlotDrawer(index):\n",
    "    print(f\"Show results for bin #{index}\")\n",
    "    if loadFromFiles:\n",
    "        samples=samplesList[index]\n",
    "    else:\n",
    "        samples = resultsMCMC[index].get_chain(discard=0)\n",
    "    if use_apinitial:\n",
    "        samples=convertToAP(samples)\n",
    "        \n",
    "    numsteps = len(samples)\n",
    "    numpars = samples.shape[2]\n",
    "    fig, axes = plt.subplots(len(_initial), figsize=(10, 70), sharex=True)\n",
    "    for i in range(len(_initial)):\n",
    "        ax = axes[i]\n",
    "        ax.plot(samples[:,:,i])\n",
    "        ax.set_ylabel(_initial[i])\n",
    "        ax.set_xlim(0, numsteps)\n",
    "\n",
    "    axes[-1].set_xlabel(\"step number\");\n",
    "    if showFigures:\n",
    "        fig.show()\n",
    "\n",
    "#     if loadFromFiles:\n",
    "#         samples=samplesList[index][burnin:,...]\n",
    "#     else:\n",
    "#         samples = resultsMCMC[index].get_chain(discard=burnin)\n",
    "#     if use_apinitial:\n",
    "#         samples=convertToAP(samples)\n",
    "#     fig2, axes2 = plt.subplots(len(_initial), figsize=(5, 70), sharex=False)\n",
    "#     for i in range(len(_initial)):\n",
    "#         ax = axes2[i]\n",
    "#         ax.hist(samples[:,:,i], 100,histtype='barstacked')\n",
    "#         ax.set_ylabel(_initial[i])\n",
    "#     if showFigures:\n",
    "#         fig2.show()\n",
    "\n",
    "    if loadFromFiles:\n",
    "        flatsample=samplesList[index][burnin:,...].reshape((numsteps-burnin)*numwalker,24)\n",
    "    else:\n",
    "        flatsample = resultsMCMC[index].get_chain(discard=burnin,flat=True)\n",
    "    if use_apinitial:\n",
    "        flatsample=convertToAP(flatsample)\n",
    "    varrange = []\n",
    "    for i in range(len(_initial)):\n",
    "        varrange.append((flatsample[:,i].min(),flatsample[:,i].max()))\n",
    "    print(f\"Drawing corner plot for bin #{index}\")\n",
    "    fig3 = corner.corner(flatsample,\n",
    "                        color='royalblue',\n",
    "                        bins=50,\n",
    "#                         truths=npy.mean(flatsample,axis=0),\n",
    "#                         quantiles=minuitresults.loc[index],\n",
    "                        range=varrange,\n",
    "                        labels=_initial,\n",
    "                        fill_contours=True,\n",
    "                        truth_color='red',\n",
    "                        label_kwargs={'fontsize':20, 'labelpad':20},\n",
    "                        hist_kwargs = {'histtype':'stepfilled','alpha':1})\n",
    "    if showFigures:\n",
    "        fig3.show()\n",
    "\n",
    "    # Draw mean and minuit values (from emcee documentation)\n",
    "    # Extract the axes\n",
    "    axes = npy.array(fig3.axes).reshape((numpars, numpars))\n",
    "\n",
    "    # Loop over the diagonal\n",
    "    for i in range(numpars):\n",
    "        ax = axes[i, i]\n",
    "#         ax.axvline(npy.mean(flatsample,axis=0)[i], color=\"k\")\n",
    "        ax.axvline(_results.loc[index][i], color=\"r\")\n",
    "\n",
    "    # Loop over the histograms\n",
    "    for yi in range(numpars):\n",
    "        for xi in range(yi):\n",
    "            ax = axes[yi, xi]\n",
    "#             ax.axvline(npy.mean(flatsample,axis=0)[xi], color=\"k\")\n",
    "            ax.axvline(_results.loc[index][xi], color=\"r\")\n",
    "#             ax.axhline(npy.mean(flatsample,axis=0)[yi], color=\"k\")\n",
    "            ax.axhline(_results.loc[index][yi], color=\"r\")\n",
    "#             ax.plot(npy.mean(flatsample,axis=0)[xi], npy.mean(flatsample,axis=0)[yi], \"sk\")\n",
    "            ax.plot(_results.loc[index][xi], _results.loc[index][yi], \"sr\")\n",
    "    \n",
    "    #######################################################\n",
    "    #################### SAVE THE FIGURES #################\n",
    "    #######################################################\n",
    "    print(f\"Saving plots for bin #{index}\")\n",
    "    fig.savefig(f\"{plotpath}/{saveFolder}/timeline_mix_bin{index}.pdf\")\n",
    "    #fig2.savefig(f\"{plotpath}/{saveFolder}/hist_mix_bin{index}.pdf\")\n",
    "    fig3.savefig(f\"{plotpath}/{saveFolder}/corner_bin{index}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFolder(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if os.path.isdir(path):\n",
    "            print(f\"Directory {path} already exists.\")\n",
    "        else:\n",
    "            print (f\"Creation of the directory {path} failed.\")\n",
    "    else:\n",
    "        print (f\"Successfully created the directory {path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folders for output\n",
    "plotpath=f\"{outputpath}\"\n",
    "saveFolder=\"diagnostics_ap\"\n",
    "makeFolder(f\"{plotpath}\")\n",
    "makeFolder(f\"{plotpath}/{saveFolder}\")\n",
    "\n",
    "# Configuration for plots\n",
    "burnin=500\n",
    "nthreads=len(binsda)\n",
    "loadFromFiles=True\n",
    "showFigures=False\n",
    "\n",
    "# Whether to show the amp/phase or re/im plots\n",
    "_results=apminuitresults\n",
    "_initial=apinitial\n",
    "use_apinitial=True\n",
    "\n",
    "print(\"Starting pool of workers to draw...\")\n",
    "globalStart = time.time()\n",
    "with Pool(nthreads) as p:\n",
    "    p.map(spawnPlotDrawer,range(len(binsda)))\n",
    "globalEnd = time.time()\n",
    "\n",
    "print(\"Elapsed time: {}\".format(globalEnd-globalStart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##############################\n",
    "# Calculate Moments\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "# H000,H010,H011,H020,H021,H022,H100,H110,H111,H120,H121,H122,sigma4,sigmay = amp.calculate_moments_JPAC(result.values)\n",
    "#H00,H11,H10,H20,H21,H22 = amp.calculate_moments_STD()\n",
    "\n",
    "\n",
    "##############################\n",
    "##############################\n",
    "# Calculate the phase motion\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "# phase = npy.empty(len(binsda))\n",
    "# for index, the_bin, result in zip(range(len(binsda)), binsda, results):\n",
    "#     amp.setup(the_bin)\n",
    "#     phase[index] = amp.Phasediff(result.values,wave[0],wave[1])\n",
    "\n",
    "# mnip = npy.empty(len(bmass), dtype=[(\"mass\", float), (\"intp\", float)])\n",
    "# mnip[\"mass\"] = bmass\n",
    "# mnip[\"intp\"] = phase\n",
    "# mnip = pandas.DataFrame(mnip)\n",
    "# countsp, bin_edges = npy.histogram(mnip[\"mass\"], nbins, weights=mnip[\"intp\"])\n",
    "\n",
    "# centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# # Add yerr to argment list when we have errors\n",
    "# yerr = npy.empty(nbins)\n",
    "# yerr = npy.sqrt(npy.abs(countsp))\n",
    "# plt.errorbar(centers,countsp, yerr, fmt=\"o\")\n",
    "# #plt.xlim(.6, 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypwa",
   "language": "python",
   "name": "pypwa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
